{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from DeepAgent.agents.DQN import DQNAgent\n",
    "from DeepAgent.agents.DoubleDQN import DoubleDQNAgent\n",
    "from DeepAgent.agents.DQN import DQNAgent\n",
    "\n",
    "from DeepAgent.networks.dueling import build_dueling_network\n",
    "from DeepAgent.networks.cnn import build_dqn_network\n",
    "\n",
    "from DeepAgent.utils.buffer import ExperienceReplay, PrioritizedExperienceReplay\n",
    "from DeepAgent.utils.game import GameEnv\n",
    "\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "test_env = GameEnv(env_name=ENV_NAME, output_shape=IMAGE_SHAPE, frame_stack=FRAME_STACK, train=False)\n",
    "evaluation_episode = 100"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def wrap_agent(agent, network, buffer):\n",
    "\n",
    "\n",
    "    _buffer = buffer(size=TEST_BUFFER_SIZE, batch_size=TEST_BATCH_SIZE)\n",
    "\n",
    "    agent = agent(\n",
    "        env=test_env,\n",
    "        model=network,\n",
    "        buffer=_buffer,\n",
    "    )\n",
    "\n",
    "    return agent\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluate DQN\n",
    "## Base DQN agent with ExperienceReply + CNN\n",
    "#### 1. Build CNN agent"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 84, 84, 4)]  0           []                               \n",
      "                                                                                                  \n",
      " lambda_6 (Lambda)              (None, 84, 84, 4)    0           ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 20, 20, 32)   8192        ['lambda_6[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 9, 9, 64)     32768       ['conv2d_9[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 7, 7, 64)     36864       ['conv2d_10[0][0]']              \n",
      "                                                                                                  \n",
      " flatten_3 (Flatten)            (None, 3136)         0           ['conv2d_11[0][0]']              \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 512)          1606144     ['flatten_3[0][0]']              \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 6)            3078        ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " lambda_7 (Lambda)              (None, 1)            0           ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 1)            513         ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " subtract_3 (Subtract)          (None, 6)            0           ['dense_11[0][0]',               \n",
      "                                                                  'lambda_7[0][0]']               \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 6)            0           ['dense_10[0][0]',               \n",
      "                                                                  'subtract_3[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,687,559\n",
      "Trainable params: 1,687,559\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dqn_network = build_dueling_network(\n",
    "        n_actions=test_env.action_space.n,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        input_shape=IMAGE_SHAPE,\n",
    "        frame_stack=FRAME_STACK\n",
    "    )\n",
    "\n",
    "dqn_agent = wrap_agent(DQNAgent, dqn_network, ExperienceReplay)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2. Evaluate with DQN saving model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zwill/miniforge3/envs/DeepRL_DemonAttack/lib/python3.9/site-packages/gym/envs/atari/environment.py:267: UserWarning: \u001B[33mWARN: We strongly suggest supplying `render_mode` when constructing your environment, e.g., gym.make(ID, render_mode='human'). Using `render_mode` provides access to proper scaling, audio support, and proper framerates.\u001B[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Episode Reward: 2970.0\n",
      "Episode: 1, Episode Reward: 940.0\n",
      "Episode: 2, Episode Reward: 2390.0\n",
      "Episode: 3, Episode Reward: 1440.0\n",
      "Episode: 4, Episode Reward: 900.0\n",
      "Episode: 5, Episode Reward: 2180.0\n",
      "Episode: 6, Episode Reward: 2690.0\n",
      "Episode: 7, Episode Reward: 1500.0\n",
      "Episode: 8, Episode Reward: 6020.0\n",
      "Episode: 9, Episode Reward: 4990.0\n",
      "Episode: 10, Episode Reward: 3090.0\n",
      "Episode: 11, Episode Reward: 310.0\n",
      "Episode: 12, Episode Reward: 4510.0\n",
      "Episode: 13, Episode Reward: 80.0\n",
      "Episode: 14, Episode Reward: 1660.0\n",
      "Episode: 15, Episode Reward: 580.0\n",
      "Episode: 16, Episode Reward: 385.0\n",
      "Episode: 17, Episode Reward: 2655.0\n",
      "Episode: 18, Episode Reward: 280.0\n",
      "Episode: 19, Episode Reward: 1820.0\n",
      "Episode: 20, Episode Reward: 355.0\n",
      "Episode: 21, Episode Reward: 1540.0\n",
      "Episode: 22, Episode Reward: 1420.0\n",
      "Episode: 23, Episode Reward: 860.0\n",
      "Episode: 24, Episode Reward: 325.0\n",
      "Episode: 25, Episode Reward: 2605.0\n",
      "Episode: 26, Episode Reward: 295.0\n",
      "Episode: 27, Episode Reward: 4090.0\n",
      "Episode: 28, Episode Reward: 960.0\n",
      "Episode: 29, Episode Reward: 2980.0\n",
      "Episode: 30, Episode Reward: 2670.0\n",
      "Episode: 31, Episode Reward: 780.0\n",
      "Episode: 32, Episode Reward: 1740.0\n",
      "Episode: 33, Episode Reward: 310.0\n",
      "Episode: 34, Episode Reward: 4015.0\n",
      "Episode: 35, Episode Reward: 100.0\n",
      "Episode: 36, Episode Reward: 355.0\n",
      "Episode: 37, Episode Reward: 900.0\n",
      "Episode: 38, Episode Reward: 4240.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [21]\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m dqn_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDQN_v1\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m      2\u001B[0m dqn_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodels/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdqn_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m----> 3\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[43mdqn_agent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mplay\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtest_env\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtest_env\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msaving_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdqn_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvideo_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m./video/\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mdqn_id\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mrender\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_episode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mevaluation_episode\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/DeepRL_DemonAttack/DeepAgent/interfaces/IBaseAgent.py:410\u001B[0m, in \u001B[0;36mBaseAgent.play\u001B[0;34m(self, test_env, saving_path, render, video_dir, frame_dir, frame_delay, max_episode, frame_frequency)\u001B[0m\n\u001B[1;32m    408\u001B[0m state \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mexpand_dims(state, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m    409\u001B[0m action \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39margmax(model(state))\n\u001B[0;32m--> 410\u001B[0m state, reward, done, _ \u001B[38;5;241m=\u001B[39m \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    411\u001B[0m episode_reward \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m reward\n\u001B[1;32m    412\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m done:\n",
      "File \u001B[0;32m~/miniforge3/envs/DeepRL_DemonAttack/lib/python3.9/site-packages/gym/wrappers/monitor.py:48\u001B[0m, in \u001B[0;36mMonitor.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep\u001B[39m(\u001B[38;5;28mself\u001B[39m, action):\n\u001B[1;32m     47\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_before_step(action)\n\u001B[0;32m---> 48\u001B[0m     observation, reward, done, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     done \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_after_step(observation, reward, done, info)\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m observation, reward, done, info\n",
      "File \u001B[0;32m~/PycharmProjects/DeepRL_DemonAttack/DeepAgent/utils/game.py:222\u001B[0m, in \u001B[0;36mGameEnv.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m    212\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep\u001B[39m(\u001B[38;5;28mself\u001B[39m, action):\n\u001B[1;32m    213\u001B[0m     \u001B[38;5;124;03m\"\"\"Performs an action and observes the result\u001B[39;00m\n\u001B[1;32m    214\u001B[0m \u001B[38;5;124;03m    Arguments:\u001B[39;00m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;124;03m        action: An integer describe action the agent chose\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    220\u001B[0m \u001B[38;5;124;03m        info: other information\u001B[39;00m\n\u001B[1;32m    221\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 222\u001B[0m     next_state, reward, done, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    223\u001B[0m     next_state \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(next_state)\n\u001B[1;32m    224\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m next_state, reward, done, info\n",
      "File \u001B[0;32m~/PycharmProjects/DeepRL_DemonAttack/DeepAgent/utils/game.py:138\u001B[0m, in \u001B[0;36mStackFrameEnv.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m    137\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep\u001B[39m(\u001B[38;5;28mself\u001B[39m, action):\n\u001B[0;32m--> 138\u001B[0m     ob, reward, done, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    139\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mframes\u001B[38;5;241m.\u001B[39mappend(ob)\n\u001B[1;32m    140\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_ob(), reward, done, info\n",
      "File \u001B[0;32m~/PycharmProjects/DeepRL_DemonAttack/DeepAgent/utils/game.py:25\u001B[0m, in \u001B[0;36mPendingFire.step\u001B[0;34m(self, ac)\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep\u001B[39m(\u001B[38;5;28mself\u001B[39m, ac):\n\u001B[0;32m---> 25\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mac\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/DeepRL_DemonAttack/DeepAgent/utils/game.py:167\u001B[0m, in \u001B[0;36mResizeEnv.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m    157\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep\u001B[39m(\u001B[38;5;28mself\u001B[39m, action):\n\u001B[1;32m    158\u001B[0m     \u001B[38;5;124;03m\"\"\"Performs an action and observes the result\u001B[39;00m\n\u001B[1;32m    159\u001B[0m \u001B[38;5;124;03m    Arguments:\u001B[39;00m\n\u001B[1;32m    160\u001B[0m \u001B[38;5;124;03m        action: An integer describe action the agent chose\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    165\u001B[0m \u001B[38;5;124;03m        info: other information\u001B[39;00m\n\u001B[1;32m    166\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 167\u001B[0m     next_state, reward, done, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    168\u001B[0m     next_state \u001B[38;5;241m=\u001B[39m process_frame(next_state)\n\u001B[1;32m    169\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m next_state, reward, done, info\n",
      "File \u001B[0;32m~/PycharmProjects/DeepRL_DemonAttack/DeepAgent/utils/game.py:102\u001B[0m, in \u001B[0;36mMaxAndSkipEnv.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m    100\u001B[0m done \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    101\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_skip):\n\u001B[0;32m--> 102\u001B[0m     obs, reward, done, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    103\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_skip \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m2\u001B[39m:\n\u001B[1;32m    104\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_obs_buffer[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m=\u001B[39m obs\n",
      "File \u001B[0;32m~/PycharmProjects/DeepRL_DemonAttack/DeepAgent/utils/game.py:86\u001B[0m, in \u001B[0;36mNoopResetEnv.step\u001B[0;34m(self, ac)\u001B[0m\n\u001B[1;32m     85\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep\u001B[39m(\u001B[38;5;28mself\u001B[39m, ac):\n\u001B[0;32m---> 86\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mac\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/DeepRL_DemonAttack/lib/python3.9/site-packages/gym/wrappers/time_limit.py:18\u001B[0m, in \u001B[0;36mTimeLimit.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep\u001B[39m(\u001B[38;5;28mself\u001B[39m, action):\n\u001B[1;32m     15\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m (\n\u001B[1;32m     16\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_elapsed_steps \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     17\u001B[0m     ), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot call env.step() before calling reset()\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m---> 18\u001B[0m     observation, reward, done, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     19\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_elapsed_steps \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m     20\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_elapsed_steps \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_max_episode_steps:\n",
      "File \u001B[0;32m~/miniforge3/envs/DeepRL_DemonAttack/lib/python3.9/site-packages/gym/envs/atari/environment.py:222\u001B[0m, in \u001B[0;36mAtariEnv.step\u001B[0;34m(self, action_ind)\u001B[0m\n\u001B[1;32m    220\u001B[0m reward \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n\u001B[1;32m    221\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(frameskip):\n\u001B[0;32m--> 222\u001B[0m     reward \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43male\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mact\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    224\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_obs(), reward, terminal, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_info()\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "dqn_id = 'DQN_v1'\n",
    "dqn_path = f'models/{dqn_id}'\n",
    "res = dqn_agent.play(test_env=test_env, saving_path=dqn_path, video_dir=f'./video/{dqn_id}',render=True, max_episode=evaluation_episode)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}